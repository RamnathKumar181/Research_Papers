# A Neural Algorithm of Artistic Style

The paper's proposes a novel artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. Furthermore, the key finding of this paper is that the representations of content and style in the Convolutional Neural Network are separable. That is, we can manipulate both representations independently to produce new, perceptually meaningful images. This can be explained as follows: when learning object recognition, the network has to become invariant to all image variation that preserves object identity. Representations that factorise the variation in the content of an image and the variation in its appearance would be extremely practical for this task. Thus, our ability to abstract content from style and therefore our ability to create and enjoy art might be primarily a preeminent signature of the powerful inference capabilities of our visual system.

## Content Loss

The results presented in this paper were generated by training a VGG Network without any fully connected layers. For image synthesis, replacing the
max-pooling operation by average pooling improves the gradient flow and one obtains slightly more appealing results.So let p ~ and ~x be the original image and the image that is generated and P_l and F_l their respective feature representation in layer l. We define the content loss as: L_content = 0.5*(F_ij^l - P_ij^l)^2 (summed over all i,j for a given layer l). This allows the system to maintatin the semantic similarity with the original image.

## Gram Matrix

These feature correlations are given by the Gram matrix G_l where G_ij^l = F_ik^l*F_jk^l (summed over all k).

## Style Loss

To generate a texture that matches the style of a given image, we use gradient descent from a white noise image to find another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. So let ~a and ~x be the original image and the image that is generated and A_l and G_l their respective style representations in layer l. The contribution of that layer to the total loss is then: E_l = (1/4*N_l^2*M_l^2) * (G_ij^l - A_ij^l)^2, where N_l is number of distinct filters and M_l is the size of each filter. The total style loss is defined as: L_style = w_l*E_l (for all layers of l). The w_l is a hyperparameter in this case.

## Combining Losses

The total loss is defined as weighted sum of the content loss and style loss. These weights are also hyperparameters.

## Paper link

https://arxiv.org/pdf/1508.06576.pdf
